{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0864128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_chat_model_cpu(model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cpu\")\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "with open(r\"C:\\Users\\kuzey\\OneDrive\\Masa√ºst√º\\CurleysWife\\data\\belief.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    beliefs = f.read().strip()\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "def safe_chat_cpu(user_input, tokenizer, model, max_tokens=100):\n",
    "    global chat_history\n",
    "\n",
    "    prompt = \"[Memory]\\n\" + beliefs + \"\\n\\n[Chat History]\\n\"\n",
    "    for turn in chat_history:\n",
    "        prompt += f\"User: {turn['user']}\\nCurley's Wife: {turn['bot']}\\n\"\n",
    "    prompt += f\"User: {user_input}\\nCurley's Wife:\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024 - max_tokens)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.75,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    reply = decoded.split(\"Curley's Wife:\")[-1].strip().split(\"\\n\")[0]\n",
    "\n",
    "    chat_history.append({\"user\": user_input, \"bot\": reply})\n",
    "    return reply\n",
    "\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext, filedialog\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pyttsx3\n",
    "\n",
    "MODEL_DIRS = {\n",
    "    \"GPT2-Medium\": r\"C:\\Users\\kuzey\\OneDrive\\Masa√ºst√º\\CurleysWife\\model\\curleyswife_gpt2medium\",\n",
    "    \"DistilGPT2\": r\"C:\\Users\\kuzey\\OneDrive\\Masa√ºst√º\\CurleysWife\\model\\curleyswife_distilgpt2\"\n",
    "}\n",
    "BELIEF_PATH = r\"C:\\Users\\kuzey\\OneDrive\\Masa√ºst√º\\CurleysWife\\data\\belief.txt\"\n",
    "\n",
    "with open(BELIEF_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    BELIEF = f.read().strip()\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "def speak(text):\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def load_model(model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "current_tokenizer, current_model = load_model(MODEL_DIRS[\"GPT2-Medium\"])\n",
    "history = []\n",
    "\n",
    "def chat(user_input, tokenizer, model):\n",
    "    global history\n",
    "    prompt = \"[Memory]\\n\" + BELIEF + \"\\n\\n[Chat History]\\n\"\n",
    "    for msg in history:\n",
    "        prompt += f\"User: {msg['user']}\\nCurley's Wife: {msg['bot']}\\n\"\n",
    "    prompt += f\"User: {user_input}\\nCurley's Wife:\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.75,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    reply = decoded.split(\"Curley's Wife:\")[-1].strip().split(\"\\n\")[0]\n",
    "    history.append({\"user\": user_input, \"bot\": reply})\n",
    "    return reply\n",
    "\n",
    "    user_input = entry.get()\n",
    "    if not user_input.strip():\n",
    "        return\n",
    "\n",
    "    root.title(\"Thinking...\")\n",
    "\n",
    "    response = chat(user_input, current_tokenizer, current_model)\n",
    "\n",
    "    root.title(\"Chat with Curley's Wife\")\n",
    "\n",
    "    chat_window.insert(tk.END, \"You: \" + user_input + \"\\n\", \"user\")\n",
    "    chat_window.insert(tk.END, \"Curley's Wife: \" + response + \"\\n\\n\", \"bot\")\n",
    "    chat_window.see(tk.END)\n",
    "    speak(response)\n",
    "    entry.delete(0, tk.END)\n",
    "\n",
    "def change_model(new_model_name):\n",
    "    global current_tokenizer, current_model, history\n",
    "    current_tokenizer, current_model = load_model(MODEL_DIRS[new_model_name])\n",
    "    history = []\n",
    "    chat_window.insert(tk.END, f\"\\nüîÅ Switched to **{new_model_name}**\\n\\n\", \"switch\")\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Chat with Curley's Wife\")\n",
    "\n",
    "frame = tk.Frame(root, bg=\"#222\")\n",
    "frame.pack(padx=10, pady=10)\n",
    "\n",
    "model_switch = tk.StringVar(root)\n",
    "model_switch.set(\"GPT2-Medium\")\n",
    "model_menu = tk.OptionMenu(frame, model_switch, *MODEL_DIRS.keys(), command=change_model)\n",
    "model_menu.config(bg=\"gray20\", fg=\"white\")\n",
    "model_menu.grid(row=0, column=0, padx=5, pady=5)\n",
    "\n",
    "chat_window = scrolledtext.ScrolledText(frame, width=70, height=22, bg=\"#1e1e1e\", fg=\"white\", font=(\"Courier\", 10))\n",
    "chat_window.grid(row=1, column=0, columnspan=2, padx=5, pady=5)\n",
    "\n",
    "# === Text Tag Styling ===\n",
    "chat_window.tag_config(\"user\", foreground=\"#90ee90\", font=(\"Courier\", 10, \"bold\"))  # light green\n",
    "chat_window.tag_config(\"bot\", foreground=\"#ff99cc\", font=(\"Courier\", 10))           # soft pink\n",
    "chat_window.tag_config(\"switch\", foreground=\"#00bfff\", font=(\"Courier\", 10, \"bold\"))# cyan\n",
    "\n",
    "entry = tk.Entry(frame, width=50, font=(\"Courier\", 10))\n",
    "entry.grid(row=2, column=0, padx=5, pady=5)\n",
    "\n",
    "send_button = tk.Button(frame, text=\"Send\", command=send_message, bg=\"#444\", fg=\"white\")\n",
    "send_button.grid(row=2, column=1, padx=5)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b18824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5482ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3668e86a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
